import zipfile
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from tensorflow.keras.preprocessing.image import load_img, img_to_array

# Unzip the dataset
zip_path = '/content/drive/MyDrive/archive (9) (1).zip'
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('/content/drive/MyDrive/archive (9) (1)')

# Define paths
base_dir = '/content/drive/MyDrive/archive (9) (1)/'
data_dir = os.path.join(base_dir)

# Define image dimensions
img_width, img_height = 150, 150  # Adjust as needed

# Load images and labels
images = []
labels = []

# Check the directory structure and load images
for label in ['yes', 'no']:
    img_dir = os.path.join(data_dir, label)
    print(f"Loading images from {img_dir}")
    for img_file in os.listdir(img_dir):
        img_path = os.path.join(img_dir, img_file)
        try:
            img = load_img(img_path, target_size=(img_width, img_height))
            img_array = img_to_array(img)
            images.append(img_array)
            labels.append(1 if label == 'yes' else 0)
        except Exception as e:
            print(f"Error loading image {img_path}: {e}")

# Convert to numpy arrays
images = np.array(images) / 255.0  # Normalize pixel values
labels = np.array(labels)

# Print total images loaded
print(f"Total images loaded: {len(images)}")

# Flatten the images for logistic regression
n_samples, h, w, c = images.shape
X = images.reshape(n_samples, h * w * c)  # Flatten each image
y = labels

# Split the dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a logistic regression model with standard scaler
model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))

# Train the model
model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred = model.predict(X_val)

# Calculate metrics
accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred)
recall = recall_score(y_val, y_pred)
f1 = f1_score(y_val, y_pred)

# Print metrics
print("Performance Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_val, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_val, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Tumor', 'Tumor'], yticklabels=['No Tumor', 'Tumor'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()
